{"cells":[{"cell_type":"markdown","metadata":{"id":"ImLCXm8IsSS2"},"source":["# Download the Cora data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xRN47p1SKRgP"},"outputs":[],"source":["! wget https://linqs-data.soe.ucsc.edu/public/lbc/cora.tgz\n","! tar -zxvf cora.tgz"]},{"cell_type":"markdown","metadata":{"id":"rXIYzURA4OKg"},"source":["# import modules and set random seed"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uJQYMX02_z0M"},"outputs":[],"source":["import numpy as np\n","import scipy.sparse as sp\n","import torch\n","import pandas as pd\n","import math\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import time\n","\n","seed = 0\n","\n","np.random.seed(seed)\n","torch.manual_seed(seed)\n","torch.cuda.manual_seed(seed)\n","torch.cuda.manual_seed_all(seed)\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"]},{"cell_type":"markdown","metadata":{"id":"dgOv1h7YsK-5"},"source":["# Loading and preprocessing the data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kXPHN61i9keB"},"outputs":[],"source":["def encode_onehot(labels):\n","    # The classes must be sorted before encoding to enable static class encoding.\n","    # In other words, make sure the first class always maps to index 0.\n","    classes = sorted(list(set(labels)))\n","    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in\n","                    enumerate(classes)}\n","    labels_onehot = np.array(list(map(classes_dict.get, labels)),\n","                             dtype=np.int32)\n","    return labels_onehot\n","\n","\n","def load_data(path=\"/content/cora/\", dataset=\"cora\", training_samples=140):\n","    \"\"\"Load citation network dataset (cora only for now)\"\"\"\n","    print('Loading {} dataset...'.format(dataset))\n","\n","    idx_features_labels = np.genfromtxt(\"{}{}.content\".format(path, dataset),\n","                                        dtype=np.dtype(str))\n","    features = sp.csr_matrix(idx_features_labels[:, 1:-1], dtype=np.float32)\n","    labels = encode_onehot(idx_features_labels[:, -1])\n","\n","    # build graph\n","    idx = np.array(idx_features_labels[:, 0], dtype=np.int32)\n","    idx_map = {j: i for i, j in enumerate(idx)}\n","    edges_unordered = np.genfromtxt(\"{}{}.cites\".format(path, dataset),\n","                                    dtype=np.int32)\n","    edges = np.array(list(map(idx_map.get, edges_unordered.flatten())),\n","                     dtype=np.int32).reshape(edges_unordered.shape)\n","    adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])),\n","                        shape=(labels.shape[0], labels.shape[0]),\n","                        dtype=np.float32)\n","\n","    # build symmetric adjacency matrix\n","    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n","\n","    features = normalize(features)\n","    adj = adj + sp.eye(adj.shape[0])\n","    adj = normalize_adj(adj)\n","\n","    # Random indexes\n","    idx_rand = torch.randperm(len(labels))\n","    # Nodes for training\n","    idx_train = idx_rand[:training_samples]\n","    # Nodes for validation\n","    idx_val= idx_rand[training_samples:]\n","\n","    adj = torch.FloatTensor(np.array(adj.todense()))\n","    features = torch.FloatTensor(np.array(features.todense()))\n","    labels = torch.LongTensor(np.where(labels)[1])\n","\n","    idx_train = torch.LongTensor(idx_train)\n","    idx_val = torch.LongTensor(idx_val)\n","\n","    return adj, features, labels, idx_train, idx_val\n","\n","def normalize_adj(mx):\n","    \"\"\"symmetric normalization\"\"\"\n","    rowsum = np.array(mx.sum(1))\n","    r_inv_sqrt = np.power(rowsum, -0.5).flatten()\n","    r_inv_sqrt[np.isinf(r_inv_sqrt)] = 0.\n","    r_mat_inv_sqrt = sp.diags(r_inv_sqrt)\n","    return mx.dot(r_mat_inv_sqrt).transpose().dot(r_mat_inv_sqrt)\n","\n","def normalize(mx):\n","    \"\"\"Row-normalize sparse matrix\"\"\"\n","    rowsum = np.array(mx.sum(1))\n","    r_inv = np.power(rowsum, -1).flatten()\n","    r_inv[np.isinf(r_inv)] = 0.\n","    r_mat_inv = sp.diags(r_inv)\n","    mx = r_mat_inv.dot(mx)\n","    return mx\n","\n","\n","def accuracy(output, labels):\n","    preds = output.max(1)[1].type_as(labels)\n","    correct = preds.eq(labels).double()\n","    correct = correct.sum()\n","    return correct / len(labels)"]},{"cell_type":"markdown","metadata":{"id":"WzCZVd1JsbHr"},"source":["## check the data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KlsKjMKx8_b7"},"outputs":[],"source":["adj, features, labels, idx_train, idx_val = load_data()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mxrv21rLnpiZ"},"outputs":[],"source":["print(adj)\n","print(adj.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lWrDf0iWnpqV"},"outputs":[],"source":["print(features)\n","print(features.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TUkt2JJdsuA2"},"outputs":[],"source":["print(labels)\n","print(labels.unique())\n","print(len(labels))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iGP18jNAs1Gp"},"outputs":[],"source":["print(len(idx_train))\n","print(len(idx_val))"]},{"cell_type":"markdown","metadata":{"id":"vHqIcfH-vIic"},"source":["# Vanilla GCN for node classification\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","source":["## Define Graph Convolution layer (Your Task)\n","\n","This module takes $\\mathbf{h} = \\{ \\overrightarrow{h_1}, \\overrightarrow{h_2}, \\dots, \\overrightarrow{h_N} \\}$ where $\\overrightarrow{h_i} \\in \\mathbb{R}^F$ as input and outputs $\\mathbf{h'} = \\{ \\overrightarrow{h'_1}, \\overrightarrow{h'_2}, \\dots, \\overrightarrow{h'_N} \\}$, where $\\overrightarrow{h'_i} \\in \\mathbb{R}^{F'}$.\n","1.   perform initial transformation: $\\mathbf{s} = \\mathbf{W} \\times \\mathbf{h} ^{(l)}$\n","2.   multiply $\\mathbf{s}$ by normalized adjacency matrix: $\\mathbf{h'} = \\mathbf{A} \\times \\mathbf{s}$"],"metadata":{"id":"f48tylWyjLPE"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"M-fU8L7f41VZ"},"outputs":[],"source":["class GraphConvolution(nn.Module):\n","    \"\"\"\n","    A Graph Convolution Layer (GCN)\n","    \"\"\"\n","\n","    def __init__(self, in_features, out_features, bias=True):\n","        \"\"\"\n","        * `in_features`, $F$, is the number of input features per node\n","        * `out_features`, $F'$, is the number of output features per node\n","        * `bias`, whether to include the bias term in the linear layer. Default=True\n","        \"\"\"\n","        super(GraphConvolution, self).__init__()\n","        # TODO: initialize the weight W that maps the input feature (dim F ) to output feature (dim F')\n","        # hint: use nn.Linear()\n","        ############ Your code here ###################################\n","\n","\n","\n","        ###############################################################\n","\n","    def forward(self, input, adj):\n","        # TODO: transform input feature to output (don't forget to use the adjacency matrix \n","        # to sum over neighbouring nodes )\n","        # hint: use the linear layer you declared above. \n","        # hint: you can use torch.spmm() sparse matrix multiplication to handle the \n","        #       adjacency matrix\n","        ############ Your code here ###################################\n","\n","\n","\n","        ###############################################################\n"]},{"cell_type":"markdown","source":["## Define GCN (Your Task)\n","\n","you will implement a two-layer GCN with ReLU activation function and Dropout after the first Conv layer."],"metadata":{"id":"RxBELCxkjF6F"}},{"cell_type":"code","source":["class GCN(nn.Module):\n","    '''\n","    A two-layer GCN\n","    '''\n","    def __init__(self, nfeat, n_hidden, n_classes, dropout, bias=True):\n","        \"\"\"\n","        * `nfeat`, is the number of input features per node of the first layer\n","        * `n_hidden`, number of hidden units\n","        * `n_classes`, total number of classes for classification\n","        * `dropout`, the dropout ratio\n","        * `bias`, whether to include the bias term in the linear layer. Default=True\n","        \"\"\"\n","\n","        super(GCN, self).__init__()\n","        # TODO: Initialization\n","        # (1) 2 GraphConvolution() layers. \n","        # (2) 1 Dropout layer\n","        # (3) 1 activation function: ReLU()\n","        ############ Your code here ###################################\n","\n","\n","\n","        ###############################################################\n","\n","    def forward(self, x, adj):\n","        # TODO: the input will pass through the first graph convolution layer, \n","        # the activation function, the dropout layer, then the second graph \n","        # convolution layer. No activation function for the \n","        # last layer. Return the logits. \n","        ############ Your code here ###################################\n","\n","\n","\n","\n","        ###############################################################"],"metadata":{"id":"HtVr2cN8jD5t"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IX1d9F1G508r"},"source":["## define loss function"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HyhqJ39OCzNN"},"outputs":[],"source":["criterion = nn.CrossEntropyLoss()"]},{"cell_type":"markdown","metadata":{"id":"vXsdid6C5K1c"},"source":["## training loop"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bjlYeoFPFAWm"},"outputs":[],"source":["args = {\"training_samples\": 140,\n","        \"epochs\": 100,\n","        \"lr\": 0.01,\n","        \"weight_decay\": 5e-4,\n","        \"hidden\": 16,\n","        \"dropout\": 0.5,\n","        \"bias\": True, \n","        }\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qbx0uc-9G5vs"},"outputs":[],"source":["def train(epoch):\n","    t = time.time()\n","    model.train()\n","    optimizer.zero_grad()\n","    output = model(features, adj)\n","    loss_train = criterion(output[idx_train], labels[idx_train])\n","    acc_train = accuracy(output[idx_train], labels[idx_train])\n","    loss_train.backward()\n","    optimizer.step()\n","\n","    model.eval()\n","    output = model(features, adj)\n","\n","    loss_val = criterion(output[idx_val], labels[idx_val])\n","    acc_val = accuracy(output[idx_val], labels[idx_val])\n","    print('Epoch: {:04d}'.format(epoch+1),\n","          'loss_train: {:.4f}'.format(loss_train.item()),\n","          'acc_train: {:.4f}'.format(acc_train.item()),\n","          'loss_val: {:.4f}'.format(loss_val.item()),\n","          'acc_val: {:.4f}'.format(acc_val.item()),\n","          'time: {:.4f}s'.format(time.time() - t))\n","\n","\n","def test():\n","    model.eval()\n","    output = model(features, adj)\n","    loss_test = criterion(output[idx_val], labels[idx_val])\n","    acc_test = accuracy(output[idx_val], labels[idx_val])\n","    print(\"Test set results:\",\n","          \"loss= {:.4f}\".format(loss_test.item()),\n","          \"accuracy= {:.4f}\".format(acc_test.item()))\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TjNiui83FYBr"},"outputs":[],"source":["model = GCN(nfeat=features.shape[1],\n","            n_hidden=args[\"hidden\"],\n","            n_classes=labels.max().item() + 1,\n","            dropout=args[\"dropout\"]).to(device)\n","optimizer = optim.Adam(model.parameters(),\n","                       lr=args[\"lr\"], weight_decay=args[\"weight_decay\"])\n","\n","\n","adj, features, labels, idx_train, idx_val = load_data(training_samples=args[\"training_samples\"])\n","adj, features, labels, idx_train, idx_val = adj.to(device), features.to(device), labels.to(device), idx_train.to(device), idx_val.to(device)"]},{"cell_type":"markdown","source":["## training Vanilla GCN"],"metadata":{"id":"1W6tqqj16iz-"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"WSjUYJPSlnOU"},"outputs":[],"source":["# Train model\n","t_total = time.time()\n","for epoch in range(args[\"epochs\"]):\n","    train(epoch)\n","print(\"Optimization Finished!\")\n","print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n","\n","# evaluating\n","test()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZF3eM6DhHfE_"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XCFwzVLmPXnH"},"outputs":[],"source":[""]},{"cell_type":"markdown","source":["# Graph Attention Networks"],"metadata":{"id":"mKHEyXp1EVdo"}},{"cell_type":"markdown","metadata":{"id":"lx15HdotKnt_"},"source":["## Graph attention layer (Your task)\n","A GAT is made up of multiple such layers. In this section, you will implement a single graph attention layer. Similar to the `GraphConvolution()`, this `GraphAttentionLayer()` module takes $\\mathbf{h} = \\{ \\overrightarrow{h_1}, \\overrightarrow{h_2}, \\dots, \\overrightarrow{h_N} \\}$ where $\\overrightarrow{h_i} \\in \\mathbb{R}^F$ as input and outputs $\\mathbf{h'} = \\{ \\overrightarrow{h'_1}, \\overrightarrow{h'_2}, \\dots, \\overrightarrow{h'_N} \\}$, where $\\overrightarrow{h'_i} \\in \\mathbb{R}^{F'}$. However, instead of weighing each neighbouring node based on the adjacency matrix, we will use self attention to learn the relative importance of each neighbouring node. Recall from HW4 where you are asked to write out the equation for single headed attention, here we will implement multi-headed attention, which involves the following steps: \n","\n","\n","### The initial transformation\n","In GCN above, you have completed similar transformation. But here, we need to define a weight matrix and perform this transformation for each head: $\\overrightarrow{s^k_i} = \\mathbf{W}^k \\overrightarrow{h_i}$. We will perform a single linear transformation and then split it up for each head later. Note the input $\\overrightarrow{h}$ has shape `[n_nodes, in_features]` and $\\overrightarrow{s}$ has shape of `[n_nodes, n_heads * n_hidden]`. Remember to reshape $\\overrightarrow{s}$ has shape of `[n_nodes, n_heads, n_hidden]` for later uses. Note: set `bias=False` for this linear transformation. \n","\n","### attention score\n","We calculate these for each head $k$. Here for simplicity of the notation, we omit $k$ in the following equations. The attention scores are defined as the follows: \n","$$e_{ij} = a(\\mathbf{W} \\overrightarrow{h_i}, \\mathbf{W} \\overrightarrow{h_j}) =a(\\overrightarrow{s_i}, \\overrightarrow{s_j})$$, \n","where $e_{ij}$ is the attention score (importance) of node $j$ to node $i$.\n","We will have to calculate this for each head. $a$ is the attention mechanism, that calculates the attention score. The paper concatenates $\\overrightarrow{s_i}$, $\\overrightarrow{s_j}$ and does a linear transformation with a weight vector $\\mathbf{a} \\in \\mathbb{R}^{2 F'}$ followed by a $\\text{LeakyReLU}$. $$e_{ij} = \\text{LeakyReLU} \\Big(\n","\\mathbf{a}^\\top \\Big[ \\overrightarrow{s_i} \\Vert \\overrightarrow{s_j}  \\Big] \\Big)$$\n","\n","#### How to vectorize this? Some hints: \n","1. `tensor.repeat()` gives you $\\{\\overrightarrow{s_1}, \\overrightarrow{s_2}, \\dots, \\overrightarrow{s_N}, \\overrightarrow{s_1}, \\overrightarrow{s_2}, \\dots, \\overrightarrow{s_N}, ...\\}$.\n","\n","2. `tensor.repeat_interleave()` gives you\n","$\\{\\overrightarrow{s_1}, \\overrightarrow{s_1}, \\dots, \\overrightarrow{s_1}, \\overrightarrow{s_2}, \\overrightarrow{s_2}, \\dots, \\overrightarrow{s_2}, ...\\}$.\n","\n","3. concatenate to get $\\Big[\\overrightarrow{s_i} \\Vert \\overrightarrow{s_j} \\Big]$ for all pairs of $i, j$. Reshape $\\overrightarrow{s_i} \\Vert \\overrightarrow{s_j}$ has shape of `[n_nodes, n_nodes, n_heads, 2 * n_hidden]`\n","\n","4. apply the attention layer and non-linear activation function to get $e_{ij} = \\text{LeakyReLU} \\Big( \\mathbf{a}^\\top \\Big[ \\overrightarrow{s_i} \\Vert \\overrightarrow{s_j}  \\Big] \\Big)$, where $\\mathbf{a}^\\top$ is a single linear transformation that maps from dimension `n_hidden * 2` to `1`. Note: set the `bias=False` for this linear transformation. $\\mathbf{e}$ is of shape `[n_nodes, n_nodes, n_heads, 1]`. Remove the last dimension `1` using `squeeze()`. \n","\n","\n","#### Perform softmax \n","First, we need to mask $e_{ij}$ based on adjacency matrix. We only need to sum over the neighbouring nodes for the attention calculation. Set the elements in $e_{ij}$ to $- \\infty$ if there is no edge from $i$ to $j$ for the softmax calculation. We need to do this for all heads and the adjacency matrix is the same for each head. Use `tensor.masked_fill()` to mask $e_{ij}$ based on adjacency matrix for all heads. Hint: reshape the adjacency matrix to `[n_nodes, n_nodes, 1]` using `unsqueeze()`. \n","Now we are ready to normalize attention scores (or coefficients) $$\\alpha_{ij} = \\text{softmax}_j(e_{ij}) =  \\frac{\\exp(e_{ij})}{\\sum_{k \\in \\mathcal{N}_i} \\exp(e_{ik})}$$\n","\n","#### Apply dropout\n","Apply the dropout layer. (this step is easy)\n","\n","#### Calculate final output for each head\n","$$\\overrightarrow{h'^k_i} = \\sum_{j \\in \\mathcal{N}_i} \\alpha^k_{ij} \\overrightarrow{s^k_j}$$\n","\n","\n","#### Concat or Mean\n","Finally we concateneate the transformed features: $\\overrightarrow{h'_i} = \\Bigg\\Vert_{k=1}^{K} \\overrightarrow{h'^k_i}$. In the code, we only need to reshape the tensor to shape of `[n_nodes, n_heads * n_hidden]`. Note that if it is the final layer, then it doesn't make sense to do concatenation anymore. Instead, we sum over the `n_heads` dimension: $\\overrightarrow{h'_i} = \\frac{1}{K} \\sum_{k=1}^{K} \\overrightarrow{h'^k_i}$. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wVu7rcOuAUZz"},"outputs":[],"source":["class GraphAttentionLayer(nn.Module):\n","\n","    def __init__(self, in_features: int, out_features: int, n_heads: int,\n","                 is_concat: bool = True,\n","                 dropout: float = 0.6,\n","                 alpha: float = 0.2):\n","        \"\"\"\n","        in_features: F, the number of input features per node\n","        out_features: F', the number of output features per node\n","        n_heads: K, the number of attention heads\n","        is_concat: whether the multi-head results should be concatenated or averaged\n","        dropout: the dropout probability\n","        alpha: the negative slope for leaky relu activation\n","        \"\"\"\n","        super(GraphAttentionLayer, self).__init__()\n","\n","        self.is_concat = is_concat\n","        self.n_heads = n_heads\n","\n","        if is_concat:\n","            assert out_features % n_heads == 0\n","            self.n_hidden = out_features // n_heads\n","        else:\n","            self.n_hidden = out_features\n","\n","        # TODO: initialize the following modules: \n","        # (1) self.W: Linear layer that transform the input feature before self attention. \n","        # You should NOT use for loops for the multiheaded implementation (set bias = Flase)\n","        # (2) self.attention: Linear layer that compute the attention score (set bias = Flase)\n","        # (3) self.activation: Activation function (LeakyReLU whith negative_slope=alpha)\n","        # (4) self.softmax: Softmax function (what's the dim to compute the summation?)\n","        # (5) self.dropout_layer: Dropout function(with ratio=dropout)\n","        ################ your code here ########################\n","\n","\n","\n","\n","\n","        ########################################################\n","\n","    def forward(self, h: torch.Tensor, adj_mat: torch.Tensor):\n","        # Number of nodes\n","        n_nodes = h.shape[0]\n","        \n","        # TODO: \n","        # (1) calculate s = Wh and reshape it to [n_nodes, n_heads, n_hidden] \n","        #     (you can use tensor.view() function)\n","        # (2) get [s_i || s_j] using tensor.repeat(), repeat_interleave(), torch.cat(), tensor.view()  \n","        # (3) apply the attention layer \n","        # (4) apply the activation layer (you will get the attention score e)\n","        # (5) remove the last dimension 1 use tensor.squeeze()\n","        # (6) mask the attention score with the adjacency matrix (if there's no edge, assign it to -inf)\n","        #     note: check the dimensions of e and your adjacency matrix. You may need to use the function unsqueeze()\n","        # (7) apply softmax \n","        # (8) apply dropout_layer \n","        ############## Your code here #########################################\n","\n","\n","\n","\n","\n","\n","        #######################################################################\n","\n","        # Summation \n","        h_prime = torch.einsum('ijh,jhf->ihf', a, s) #[n_nodes, n_heads, n_hidden]\n","\n","\n","        # TODO: Concat or Mean\n","        # Concatenate the heads\n","        if self.is_concat:\n","            ############## Your code here #########################################\n","\n","\n","            #######################################################################\n","        # Take the mean of the heads (for the last layer)\n","        else:\n","            ############## Your code here #########################################\n","\n","\n","\n","            #######################################################################\n","\n","\n","\n","\n"]},{"cell_type":"markdown","source":["## Define GAT network\n","it's really similar to how we defined GCN. We followed the paper to use two attention layers and ELU() activation function. "],"metadata":{"id":"YOSk_ZShi2nR"}},{"cell_type":"code","source":["class GAT(nn.Module):\n","\n","    def __init__(self, nfeat: int, n_hidden: int, n_classes: int, n_heads: int, dropout: float, alpha: float):\n","        \"\"\"\n","        in_features: the number of features per node\n","        n_hidden: the number of features in the first graph attention layer\n","        n_classes: the number of classes\n","        n_heads: the number of heads in the graph attention layers\n","        dropout: the dropout probability\n","        alpha: the negative input slope for leaky ReLU of the attention layer\n","        \"\"\"\n","        super().__init__()\n","\n","        # First graph attention layer where we concatenate the heads\n","        self.gc1 = GraphAttentionLayer(nfeat, n_hidden, n_heads, is_concat=True, dropout=dropout, alpha=alpha)\n","        self.gc2 = GraphAttentionLayer(n_hidden, n_classes, 1, is_concat=False, dropout=dropout, alpha=alpha)\n","        self.activation = nn.ELU()  \n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x: torch.Tensor, adj_mat: torch.Tensor):\n","        \"\"\"\n","        x: the features vectors\n","        adj_mat: the adjacency matrix\n","        \"\"\"\n","        x = self.dropout(x)\n","        x = self.gc1(x, adj_mat)\n","        x = self.activation(x)\n","        x = self.dropout(x)\n","        x = self.gc2(x, adj_mat)\n","        return x"],"metadata":{"id":"jKNbUtPVi1Vs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## training GAT"],"metadata":{"id":"CtRQ3Ced7RAw"}},{"cell_type":"code","source":["args = {\"training_samples\": 140,\n","        \"epochs\": 100,\n","        \"lr\": 0.01,\n","        \"weight_decay\": 5e-4,\n","        \"hidden\": 16,\n","        \"dropout\": 0.5,\n","        \"bias\": True, \n","        \"alpha\": 0.2,\n","        \"n_heads\": 8\n","        }"],"metadata":{"id":"b7D5mYXC6zTG"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7MYaK98hDy7u"},"outputs":[],"source":["model = GAT(nfeat=features.shape[1],\n","            n_hidden=args[\"hidden\"],\n","            n_classes=labels.max().item() + 1,\n","            dropout=args[\"dropout\"],\n","            alpha=args[\"alpha\"],\n","            n_heads=args[\"n_heads\"]).to(device)\n","optimizer = optim.Adam(model.parameters(),\n","                       lr=args[\"lr\"], weight_decay=args[\"weight_decay\"])\n","\n","adj, features, labels, idx_train, idx_val = load_data(training_samples=args[\"training_samples\"])\n","adj, features, labels, idx_train, idx_val = adj.to(device), features.to(device), labels.to(device), idx_train.to(device), idx_val.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E9FcfXwMDzEt"},"outputs":[],"source":["# Train model\n","t_total = time.time()\n","for epoch in range(args[\"epochs\"]):\n","    train(epoch)\n","print(\"Optimization Finished!\")\n","print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n","\n","# Testing\n","test()"]},{"cell_type":"markdown","source":["# Question: (Your task)\n","Compare the evaluation results for Vanilla GCN and GAT. Comment on the discrepancy in their performance (if any) and briefly explain why you think it's the case (in 1-2 sentences). "],"metadata":{"id":"n6Ox3fbTG7rc"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"urJ8Q-neDzHU"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vZhHh8k4DzJu"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KmvJ46OfGlf2"},"outputs":[],"source":[""]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"a4_GCN.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}