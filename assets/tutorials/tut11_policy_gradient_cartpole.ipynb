{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tut11_policy_gradient_cartpole.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eYla8FUsmZcD"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from torch.distributions import Bernoulli\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PolicyNet(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(PolicyNet, self).__init__()\n",
        "\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "        self.fc1 = nn.Linear(self.input_dim, 32)\n",
        "        self.fc2 = nn.Linear(32, 32)\n",
        "        self.output = nn.Linear(32, self.output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = F.relu(self.fc1(x))\n",
        "        output = F.relu(self.fc2(output))\n",
        "        output = torch.sigmoid(self.output(output))\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "8MKfIqjZm6E9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_to_torch_variable(arr):\n",
        "    \"\"\"Converts a numpy array to torch variable\"\"\"\n",
        "    return Variable(torch.from_numpy(arr).float())\n",
        "\n"
      ],
      "metadata": {
        "id": "B5cFm2RCm8UE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define environment\n",
        "env = gym.make(\"CartPole-v0\")\n",
        "env.seed(0)\n",
        "\n",
        "# Create environment monitor for video recording\n",
        "video_monitor_callable = lambda _: True\n",
        "# monitored_env = gym.wrappers.Monitor(env, './cartpole_videos', force=True, video_callable=video_monitor_callable)\n",
        "\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.n\n",
        "bernoulli_action_dim = 1\n",
        "\n",
        "# Initialize policy network\n",
        "policy_net = PolicyNet(input_dim=state_dim, output_dim=bernoulli_action_dim)\n",
        "\n",
        "# Hyperparameters\n",
        "NUM_EPISODES = 500\n",
        "GAMMA = 0.99\n",
        "BATCH_SIZE = 5\n",
        "LEARNING_RATE = 0.01\n",
        "\n",
        "# Let baseline be 0 for now\n",
        "baseline = 0.0\n",
        "\n",
        "# Define optimizer\n",
        "optimizer = torch.optim.RMSprop(policy_net.parameters(), lr=LEARNING_RATE)"
      ],
      "metadata": {
        "id": "AFtlztRdm9pv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Collect trajectory rewards for plotting purpose\n",
        "traj_reward_history = []\n",
        "\n",
        "# training loop\n",
        "for ep_i in range(NUM_EPISODES):\n",
        "    loss = 0.0\n",
        "\n",
        "    # Record states, actions and discounted rewards of this episode\n",
        "    states = []\n",
        "    actions = []\n",
        "    rewards = []\n",
        "    cumulative_undiscounted_reward = 0.0\n",
        "\n",
        "    for traj_i in range(BATCH_SIZE):\n",
        "        time_step = 0\n",
        "        done = False\n",
        "\n",
        "        # initialize environment\n",
        "        cur_state = env.reset()\n",
        "        cur_state = convert_to_torch_variable(cur_state)\n",
        "\n",
        "        discount_factor = 1.0\n",
        "        discounted_rewards = []\n",
        "\n",
        "        grad_log_params = []\n",
        "\n",
        "        while not done:\n",
        "            # Compute action probability using the current policy\n",
        "            action_prob = policy_net(cur_state)\n",
        "\n",
        "            # Sample action according to action probability\n",
        "            action_sampler = Bernoulli(probs=action_prob)\n",
        "            action = action_sampler.sample()\n",
        "            action = action.numpy().astype(int)[0]\n",
        "\n",
        "            # Record the states and actions -- will be used for policy gradient later\n",
        "            states.append(cur_state)\n",
        "            actions.append(action)\n",
        "\n",
        "            # take a step in the environment, and collect data\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "            # Discount the reward, and append to reward list\n",
        "            discounted_reward = reward * discount_factor\n",
        "            discounted_rewards.append(discounted_reward)\n",
        "            cumulative_undiscounted_reward += reward\n",
        "\n",
        "            # Prepare for taking the next step\n",
        "            cur_state = convert_to_torch_variable(next_state)\n",
        "\n",
        "            time_step += 1\n",
        "            discount_factor *= GAMMA\n",
        "\n",
        "        # Finished collecting data for the current trajectory. \n",
        "        # Recall temporal structure in policy gradient.\n",
        "        # Donstruct the \"cumulative future discounted reward\" at each time step.\n",
        "        for time_i in range(time_step):\n",
        "            # relevant reward is the sum of rewards from time t to the end of trajectory\n",
        "            relevant_reward = sum(discounted_rewards[time_i:])\n",
        "            rewards.append(relevant_reward)\n",
        "\n",
        "    # Finished collecting data for this batch. Update policy using policy gradient.\n",
        "    avg_traj_reward = cumulative_undiscounted_reward / BATCH_SIZE\n",
        "    traj_reward_history.append(avg_traj_reward)\n",
        "\n",
        "    if (ep_i + 1) % 10 == 0:\n",
        "        print(\"Episode {}: Average reward per trajectory = {}\".format(ep_i + 1, avg_traj_reward))\n",
        "\n",
        "    #if (ep_i + 1) % 100 == 0:\n",
        "    #    record_video()\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    data_len = len(states)\n",
        "    loss = 0.0\n",
        "\n",
        "    # Compute the policy gradient\n",
        "    for data_i in range(data_len):\n",
        "        action_prob = policy_net(states[data_i])\n",
        "        action_sampler = Bernoulli(probs=action_prob)\n",
        "\n",
        "        loss -= action_sampler.log_prob(torch.Tensor([actions[data_i]])) * (rewards[data_i] - baseline)\n",
        "    loss /= float(data_len)\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 554
        },
        "id": "TYTpeSRWnAOn",
        "outputId": "199fc5a0-245c-49e5-b9be-6a576bcedaae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 10: Average reward per trajectory = 123.6\n",
            "Episode 20: Average reward per trajectory = 200.0\n",
            "Episode 30: Average reward per trajectory = 200.0\n",
            "Episode 40: Average reward per trajectory = 200.0\n",
            "Episode 50: Average reward per trajectory = 200.0\n",
            "Episode 60: Average reward per trajectory = 200.0\n",
            "Episode 70: Average reward per trajectory = 200.0\n",
            "Episode 80: Average reward per trajectory = 200.0\n",
            "Episode 90: Average reward per trajectory = 200.0\n",
            "Episode 100: Average reward per trajectory = 190.6\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-f247cb1070d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0maction_sampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBernoulli\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maction_prob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0maction_sampler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata_i\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata_i\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbaseline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/distributions/bernoulli.py\u001b[0m in \u001b[0;36mlog_prob\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbroadcast_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mbinary_cross_entropy_with_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'none'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/distributions/utils.py\u001b[0m in \u001b[0;36mbroadcast_all\u001b[0;34m(*values)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \"\"\"\n\u001b[1;32m     27\u001b[0m     if not all(isinstance(v, torch.Tensor) or has_torch_function((v,)) or isinstance(v, Number)\n\u001b[0;32m---> 28\u001b[0;31m                for v in values):\n\u001b[0m\u001b[1;32m     29\u001b[0m         raise ValueError('Input arguments must all be instances of numbers.Number, '\n\u001b[1;32m     30\u001b[0m                          'torch.Tensor or objects implementing __torch_function__.')\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/distributions/utils.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \"\"\"\n\u001b[1;32m     27\u001b[0m     if not all(isinstance(v, torch.Tensor) or has_torch_function((v,)) or isinstance(v, Number)\n\u001b[0;32m---> 28\u001b[0;31m                for v in values):\n\u001b[0m\u001b[1;32m     29\u001b[0m         raise ValueError('Input arguments must all be instances of numbers.Number, '\n\u001b[1;32m     30\u001b[0m                          'torch.Tensor or objects implementing __torch_function__.')\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Don't forget to close the environments.\n",
        "#monitored_env.close()\n",
        "env.close()\n",
        "\n",
        "# Plot learning curve\n",
        "plt.figure()\n",
        "plt.plot(traj_reward_history)\n",
        "plt.title(\"Learning to Solve CartPole-v1 with Policy Gradient\")\n",
        "plt.xlabel(\"Episode\")\n",
        "plt.ylabel(\"Average Reward per Trajectory\")\n",
        "plt.savefig(\"CartPole-pg.png\")\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "metadata": {
        "id": "CxtJD3jtnChJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}