{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNNs and LSTMs for Character-Level Language Models\n",
    "\n",
    "In this tutorial, we will reproduce Andrej Karpathy's character level language model, which is described here: http://karpathy.github.io/2015/05/21/rnn-effectiveness/. This tutorial is largely based on this blog post, but has been updated to use PyTorch where applicable. \n",
    "\n",
    "The objective of this character level language model is to predict the next character given a sequence of previously observed characters. In this tutorial, we explore the ability for Recurrent Neural Networks (RNNs) and LSTMs to perform this task. Characters are converted into a one-hot vector. We can then view the softmax output of each RNN cell as a probability distribution over the possible next characters. From Karpathy's blog post, we show a visualization of the task: \n",
    "\n",
    "![title](http://karpathy.github.io/assets/rnn/charseq.jpeg)\n",
    "The output of each RNN cell is the probability distribution for the next character. We can then sample the next character using this distribution. At evaluation time, we can feed each sampled character into the RNN as an input, allowing us to generate a sequence of text.\n",
    "\n",
    "## Data Preprocessing\n",
    "In this notebook, we will be training on a dataset of Shakespearian dialogue. First, lets download the data, and inspect a sample passage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
    "filename = 'shakespeare_data.txt'\n",
    "urllib.request.urlretrieve(url, filename)\n",
    "\n",
    "\n",
    "data_file = open(filename, 'r')\n",
    "raw_data = data_file.read()\n",
    "data_file.close()\n",
    "\n",
    "print(raw_data[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will represent characters using a one hot encoding. Each character in the vocabulary is first mapped to an index, and we then define a function to map this index to a one-hot vector and vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_length = len(raw_data)\n",
    "vocab = list(set(raw_data))\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "char_to_index = { char:index for (index,char) in enumerate(vocab) }\n",
    "index_to_char = { index:char for (index,char) in enumerate(vocab) }\n",
    "\n",
    "print(\"The vocabulary contains {}\".format(vocab))\n",
    "print(\"------------------------------\")\n",
    "print(\"TOTAL NUM CHARACTERS = {}\".format(data_length))\n",
    "print(\"NUM UNIQUE CHARACTERS = {}\".format(vocab_size))\n",
    "print('char_to_index {}'.format(char_to_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial will use a simplistic method to extract sequences from this dataset. We will simply chunk the data into evenly sized sub-sequences, and discard the remaining data. This is not a good practice in reality, and may hurt performance!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "def create_one_hot(ind, length):\n",
    "    \"\"\"Convert index into one-hot vector, where the index is set to hot.\"\"\"\n",
    "    vec = np.zeros(length)\n",
    "    vec[ind] = 1\n",
    "    return vec\n",
    "\n",
    "def chunk_data(raw_data, seq_len):\n",
    "    \"\"\"Splits raw data into evenly sized chunks.\"\"\"\n",
    "    chunks = []\n",
    "\n",
    "    for i in range(len(raw_data) // seq_len):\n",
    "        start = i * seq_len\n",
    "        end = start + seq_len + 1\n",
    "        chunk = raw_data[start:end]\n",
    "\n",
    "        chunks.append(chunk)\n",
    "    return chunks\n",
    "\n",
    "def convert_dataset(dataset, char_to_index, vocab_size):\n",
    "    \"\"\"Convert dataset of character sequences into index and one hot data.\"\"\"\n",
    "    ind_dataset = []\n",
    "    one_hot_dataset = []\n",
    "\n",
    "    for seq in dataset:\n",
    "        ind_seq = [char_to_index[c] for c in seq]\n",
    "        one_hot_seq = [create_one_hot(ind, vocab_size) for ind in ind_seq]\n",
    "\n",
    "        ind_dataset.append(ind_seq)\n",
    "        one_hot_dataset.append(one_hot_seq)\n",
    "\n",
    "    return np.array(ind_dataset), np.array(one_hot_dataset)\n",
    "\n",
    "class ShakespeareDataset(Dataset):\n",
    "    def __init__(self, inds, one_hot):\n",
    "        self.inds = inds\n",
    "        self.one_hot = one_hot\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.one_hot.size(0)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Note that we offset the data here, so the target for each character\n",
    "        # is the next character in the sequence.\n",
    "        input_onehot = self.one_hot[idx, :-1, :]\n",
    "        target_ind = self.inds[idx, 1:]\n",
    "\n",
    "        return input_onehot, target_ind\n",
    "\n",
    "CHUNK_LEN = 25\n",
    "\n",
    "data_chunks = chunk_data(raw_data, CHUNK_LEN)\n",
    "train_ind, train_oh = convert_dataset(data_chunks, char_to_index, vocab_size)\n",
    "\n",
    "# Send data to GPU\n",
    "train_ind_tt = torch.Tensor(train_ind).long().to(device)\n",
    "train_oh_tt = torch.Tensor(train_oh).float().to(device)\n",
    "\n",
    "train_set = ShakespeareDataset(train_ind_tt, train_oh_tt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks\n",
    "\n",
    "Below we define our own RNN implementation. Recall that the RNN performs the following operations:\n",
    "\n",
    "\n",
    "$$ \n",
    "\\begin{align} h_t &= W_{ih} x_t + W_{hh} h_{t-1} + b_{ih} + b_{hh}\\\\\n",
    " a_t &= \\text{tanh}(h_t) \\\\\n",
    " o_t &= \\text{softmax}(W_{ho} a_t + b_{ho}) \n",
    " \\end{align} \n",
    "$$\n",
    " \n",
    " \n",
    "You may find the following resources helpful for understanding how RNNs and LSTMs work:\n",
    "\n",
    "* [The Unreasonable Effectiveness of RNNs (Andrej Karpathy)](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n",
    "* [Recurrent Neural Networks Tutorial (Wild ML)](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/)\n",
    "* [Understanding LSTM Networks (Chris Olah)](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
    " \n",
    " \n",
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyRNNCell(nn.Module):\n",
    "    def __init__(self, obs_dim, hidden_size, output_dim):\n",
    "        \"\"\"Initialize RNN Cell.\"\"\"\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Merge input / hidden weights into single module.\n",
    "        self.i2h = nn.Linear(obs_dim + hidden_size, hidden_size)\n",
    "        self.h2o = nn.Linear(hidden_size, output_dim)\n",
    "\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "    \n",
    "    def forward(self, data, hidden):\n",
    "        \"\"\"Compute forward pass for this RNN cell.\"\"\"\n",
    "        combined = torch.cat((data, hidden), 1)\n",
    "\n",
    "        hidden = self.i2h(combined)\n",
    "        hidden = self.tanh(hidden)\n",
    "\n",
    "        output = self.h2o(hidden)\n",
    "        output = self.softmax(output)\n",
    "\n",
    "        return output, hidden\n",
    "    \n",
    "class MyRNN(nn.Module):\n",
    "    def __init__(self, obs_dim, hidden_size, output_dim):\n",
    "        \"\"\"Initialize RNN.\"\"\"\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        self.rnn_cell = MyRNNCell(obs_dim, hidden_size, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Compute forward pass on sequence x.\n",
    "        \n",
    "        Input sequence x has shape (B x L x D), where:\n",
    "        B is batch size, L is sequence length, and D is the number of features.\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, n_feat = x.size()\n",
    "        \n",
    "        # Stores outputs of RNN cell\n",
    "        output_arr = torch.zeros((batch_size, seq_len, self.output_dim))\n",
    "        hidden_arr = torch.zeros((batch_size, seq_len, self.hidden_size))\n",
    "        \n",
    "        # Send to GPU. This is a gotcha, make sure to send Tensors created\n",
    "        # in a model to the same device as input Tensors.\n",
    "        output_arr = output_arr.float().to(x.device)\n",
    "        hidden_arr = hidden_arr.float().to(x.device)\n",
    "\n",
    "        hidden = self.init_hidden(batch_size, x.device)\n",
    "\n",
    "        for i in range(seq_len):\n",
    "            # For each iteration, compute RNN on input for current position\n",
    "            output, hidden = self.rnn_cell(x[:, i, :], hidden)\n",
    "\n",
    "            output_arr[:, i, :] = output\n",
    "            hidden_arr[:, i, :] = hidden\n",
    "\n",
    "        return output_arr, hidden_arr\n",
    "\n",
    "    def init_hidden(self, batch_size, device):\n",
    "        \"\"\"Initialize RNN hidden state.\n",
    "        \n",
    "        Some people advocate for using random noise instead of zeros, or \n",
    "        training for the initial state. Personally, I don't know if it matters!\n",
    "        \"\"\"\n",
    "        return torch.zeros(batch_size, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_seq(model, init_char_one_hot, length):\n",
    "    \"\"\"Generate sequence using autoregressive scheme.\n",
    "    \n",
    "    This is a little messy, but the core concept is to:\n",
    "\n",
    "      1. Get the distribution of next characters from the RNN\n",
    "      2. Use this distribution to sample the next character\n",
    "      3. Feed the sampled character into the RNN, and repeat    \n",
    "    \"\"\"\n",
    "    curr_char = init_char_one_hot\n",
    "    output = index_to_char[torch.argmax(curr_char.squeeze()).item()]\n",
    "\n",
    "    for i in range(length):\n",
    "        out, _ = model(curr_char)\n",
    "\n",
    "        # Since our output is a probability distribution, we can sample from it\n",
    "        p = np.exp(out[:, -1, :].cpu().detach().numpy())\n",
    "        out_ind = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "        \n",
    "        out_char = index_to_char[out_ind]\n",
    "        \n",
    "        output += out_char\n",
    "        \n",
    "        # Use sampled output as input for next time step\n",
    "        curr_char = create_one_hot(out_ind, vocab_size)\n",
    "        curr_char = torch.Tensor(curr_char).float().to(device).view(1, 1, -1)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(model, optimizer, train_loader, n_epochs, test_char=None):\n",
    "    for epoch in range(n_epochs):\n",
    "        avg_loss = []\n",
    "        for input_seq, target_ind in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output, _ = model(input_seq)\n",
    "            loss = nn.NLLLoss()(output.transpose(1, 2), target_ind)\n",
    "\n",
    "            loss.backward()\n",
    "            \n",
    "            # =================================================================\n",
    "            # This is how to do gradient clipping in PyTorch\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
    "            # =================================================================\n",
    "            \n",
    "            optimizer.step()\n",
    "            avg_loss.append(loss.item())\n",
    "\n",
    "        print('Epoch {} : Avg Train Loss {}'.format(epoch, np.mean(avg_loss)))\n",
    "\n",
    "        # Generate sequence\n",
    "        if test_char is not None:\n",
    "            gen_seq = generate_seq(model, test_char, 100)\n",
    "            print(\"Generated Sequence:\\n {}\".format(gen_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "HIDDEN_SIZE = 100\n",
    "N_EPOCH = 100\n",
    "LR = 0.01\n",
    "BATCH_SIZE = 64\n",
    "SAMP_CHAR = 'a'\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "model = MyRNN(vocab_size, HIDDEN_SIZE, vocab_size).to(device)\n",
    "optim = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "test_char = create_one_hot(char_to_index[SAMP_CHAR], vocab_size)\n",
    "test_char_tt = torch.Tensor(test_char).view(1, 1, -1).float().to(device)\n",
    "\n",
    "train_loop(model, optim, train_loader, N_EPOCH, test_char_tt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTMs\n",
    "\n",
    "Long short-term memory (LSTM) units contain a cell state, which allows long term dependencies to propogate through the RNN. The LSTM is represented by:\n",
    "\n",
    "$$\n",
    "        \\begin{array}{ll} \\\\\n",
    "            i_t = \\sigma(W_{ii} x_t + b_{ii} + W_{hi} h_{t-1} + b_{hi}) \\\\\n",
    "            f_t = \\sigma(W_{if} x_t + b_{if} + W_{hf} h_{t-1} + b_{hf}) \\\\\n",
    "            g_t = \\tanh(W_{ig} x_t + b_{ig} + W_{hg} h_{t-1} + b_{hg}) \\\\\n",
    "            o_t = \\sigma(W_{io} x_t + b_{io} + W_{ho} h_{t-1} + b_{ho}) \\\\\n",
    "            c_t = f_t \\odot c_{t-1} + i_t \\odot g_t \\\\\n",
    "            h_t = o_t \\odot \\tanh(c_t) \\\\\n",
    "        \\end{array}\n",
    "$$\n",
    "        \n",
    "Due to the assignment this year, we don't be showing a detailed implementation here. Instead, we use the builting PyTorch LSTM:  \n",
    "https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLSTM(nn.Module):\n",
    "    \"\"\"Wraps PyTorch NN with output network\"\"\"\n",
    "\n",
    "    def __init__(self, obs_dim, hid_size, num_layers):\n",
    "        \"\"\"Initialize MyLSTM.\"\"\"\n",
    "        super().__init__()\n",
    "        # Using built-in PyTorch LSTM, see source code for implementation.\n",
    "        self.lstm = nn.LSTM(obs_dim, hid_size, num_layers=num_layers, \n",
    "                            batch_first=True)\n",
    "\n",
    "        self.out = nn.Linear(hid_size, obs_dim)\n",
    "        self.act = nn.LogSoftmax(dim=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out = self.lstm(x)[0]\n",
    "        out = self.act(self.out(lstm_out) / 0.5)\n",
    "        return out, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "HIDDEN_SIZE = 512\n",
    "NUM_LAYERS = 3\n",
    "N_EPOCH = 10000\n",
    "\n",
    "LR = 0.001\n",
    "BATCH_SIZE = 32\n",
    "SAMP_CHAR = 'a'\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "model = MyLSTM(vocab_size, HIDDEN_SIZE, NUM_LAYERS).to(device)\n",
    "optim = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "test_char = create_one_hot(char_to_index[SAMP_CHAR], vocab_size)\n",
    "test_char_tt = torch.Tensor(test_char).view(1, 1, -1).float().to(device)\n",
    "\n",
    "train_loop(model, optim, train_loader, N_EPOCH, test_char_tt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrap-Up\n",
    "\n",
    "Apparently it takes several hours to train this model, which we obviously don't have!\n",
    "\n",
    "Refer to the website for results: http://karpathy.github.io/2015/05/21/rnn-effectiveness/. \n",
    "\n",
    "A 100,000 character sample output of the trained LSTM model can be found at: https://cs.stanford.edu/people/karpathy/char-rnn/shakespear.txt. It seems very good! We include a snippet of the output from the website below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">PANDARUS:\n",
    "Alas, I think he shall be come approached and the day\n",
    "When little srain would be attain'd into being never fed,\n",
    "And who is but a chain and subjects of his death,\n",
    "I should not sleep.\n",
    "\n",
    ">Second Senator:\n",
    "They are away this miseries, produced upon my soul,\n",
    "Breaking and strongly should be buried, when I perish\n",
    "The earth and thoughts of many states.\n",
    "\n",
    ">DUKE VINCENTIO:\n",
    "Well, your wit is in the care of side and that.\n",
    "\n",
    ">Second Lord:\n",
    "They would be ruled after this chamber, and\n",
    "my fair nues begun out of the fact, to be conveyed,\n",
    "Whose noble souls I'll have the heart of the wars.\n",
    "\n",
    ">Clown:\n",
    "Come, sir, I will make did behold your worship.\n",
    "\n",
    ">VIOLA:\n",
    "I'll drink it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}